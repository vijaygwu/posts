{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyOircPNZT+lsvc/t4VrqwV/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijaygwu/classideas/blob/main/PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**:\n",
        "- The agent (PPO) uses two neural networks: an actor (policy) and a critic (value function).\n",
        "- It interacts with an environment, collects experiences, and then trains its policy and value function based on these experiences.\n",
        "- PPO introduces a clipping mechanism in the policy update step to prevent large policy changes, ensuring more stable and robust learning.\n",
        "- The algorithm also leverages Generalized Advantage Estimation (GAE) for a better estimate of advantages, and entropy regularization to encourage exploration.\n",
        "\n",
        "The key advantage of PPO over other policy gradient methods is its balance between ease of implementation, sample efficiency, and training stability. This code captures the core essence of PPO in a modular and organized manner, making it easier to understand and potentially extend.\n",
        "\n",
        "** Implementation Details **\n",
        "\n",
        "The code is a Python implementation of the Proximal Policy Optimization (PPO) algorithm using the PyTorch library.\n",
        "\n",
        "1. **Neural Network Architecture (ActorCritic)**:\n",
        "    - **Actor**: Represents the policy of the agent. Given a state, it outputs action probabilities (for discrete action spaces) or action means (for continuous action spaces).\n",
        "    - **Critic**: Represents the value function of the agent. Given a state, it estimates its value.\n",
        "\n",
        "2. **PPO Algorithm Class (PPO)**:\n",
        "   - Initializes various parameters and the neural network (ActorCritic).\n",
        "   - `compute_gae`: Calculates the Generalized Advantage Estimation (GAE). GAE is a technique to estimate the advantage of an action in a state, which can lead to more stable and efficient learning.\n",
        "   - `ppo_step`: Performs one update step for the PPO algorithm. It updates the policy using the PPO clipping objective and also updates the value function.\n",
        "   - `train`: Given collected experience (memory), this method divides the experience into mini-batches and updates the policy and value function using the `ppo_step` method.\n",
        "\n",
        "3. **Memory Class (Memory)**:\n",
        "   - A simple class to store the experiences (state, action, value, log-probability of the action, reward, and mask indicating episode end) collected by the agent during its interaction with the environment.\n",
        "   - `append`: Appends an experience to the memory.\n",
        "   - `clear`: Clears the memory.\n",
        "\n",
        "4. **Environment Setup **:\n",
        "   -  You'd need to use the `gym` library to utilize this part of the code. The setup includes:\n",
        "      - Environment creation.\n",
        "      - Instantiating the PPO agent and memory.\n",
        "      -  In the end - You'd then loop to collect experiences using the policy, store them in memory, and train the agent using these experiences.\n",
        "\n"
      ],
      "metadata": {
        "id": "sC4lzLkG1mEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, continuous=False):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.continuous = continuous\n",
        "\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_dim)\n",
        "        )\n",
        "\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        if continuous:\n",
        "            self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "\n",
        "    def forward(self, state):\n",
        "        value = self.critic(state)\n",
        "        if self.continuous:\n",
        "            mean = self.actor(state)\n",
        "            std = torch.exp(self.log_std).unsqueeze(0).expand_as(mean)\n",
        "            action_dist = torch.distributions.Normal(mean, std)\n",
        "        else:\n",
        "            action_probs = torch.softmax(self.actor(state), dim=-1)\n",
        "            action_dist = torch.distributions.Categorical(action_probs)\n",
        "        return action_dist, value\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, continuous=False, lr=3e-4, gamma=0.99, tau=0.95, clip_epsilon=0.2, entropy_coef=0.01):\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.clip_epsilon = clip_epsilon\n",
        "        self.entropy_coef = entropy_coef\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, continuous)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "\n",
        "    def compute_gae(self, next_value, rewards, masks, values):\n",
        "        values = values + [next_value]\n",
        "        gae = 0\n",
        "        returns = []\n",
        "        for step in reversed(range(len(rewards))):\n",
        "            delta = rewards[step] + self.gamma * values[step + 1] * masks[step] - values[step]\n",
        "            gae = delta + self.gamma * self.tau * masks[step] * gae\n",
        "            returns.insert(0, gae + values[step])\n",
        "        return returns\n",
        "\n",
        "    def ppo_step(self, states, actions, log_probs, returns, advantages):\n",
        "        action_dist, values = self.policy(states)\n",
        "        entropy = action_dist.entropy().mean()\n",
        "        new_log_probs = action_dist.log_prob(actions)\n",
        "\n",
        "        ratio = (new_log_probs - log_probs).exp()\n",
        "        surr1 = ratio * advantages\n",
        "        surr2 = torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon) * advantages\n",
        "\n",
        "        actor_loss = -torch.min(surr1, surr2).mean()\n",
        "        critic_loss = 0.5 * (returns - values).pow(2).mean()\n",
        "\n",
        "        loss = actor_loss + critic_loss - self.entropy_coef * entropy\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def train(self, memory, ppo_epochs=4, mini_batch_size=64):\n",
        "        states = torch.tensor(memory.states, dtype=torch.float32)\n",
        "        actions = torch.tensor(memory.actions)\n",
        "        old_log_probs = torch.tensor(memory.log_probs)\n",
        "\n",
        "        rewards = torch.tensor(memory.rewards)\n",
        "        masks = torch.tensor(memory.masks)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, next_value = self.policy(states[-1])\n",
        "        returns = self.compute_gae(next_value, memory.rewards, memory.masks, memory.values)\n",
        "\n",
        "        returns_tensor = torch.tensor(returns, dtype=torch.float32)\n",
        "        values_tensor = torch.tensor(memory.values, dtype=torch.float32)\n",
        "        advantages = returns_tensor - values_tensor\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        for _ in range(ppo_epochs):\n",
        "            for index in range(0, len(states), mini_batch_size):\n",
        "                sampled_states = states[index:index+mini_batch_size]\n",
        "                sampled_actions = actions[index:index+mini_batch_size]\n",
        "                sampled_old_log_probs = old_log_probs[index:index+mini_batch_size]\n",
        "                sampled_returns = returns_tensor[index:index+mini_batch_size]\n",
        "                sampled_advantages = advantages[index:index+mini_batch_size]\n",
        "\n",
        "                self.ppo_step(sampled_states, sampled_actions, sampled_old_log_probs, sampled_returns, sampled_advantages)\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.values = []\n",
        "        self.log_probs = []\n",
        "        self.rewards = []\n",
        "        self.masks = []\n",
        "\n",
        "    def append(self, state, action, value, log_prob, reward, mask):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.values.append(value.item())\n",
        "        self.log_probs.append(log_prob.item())\n",
        "        self.rewards.append(reward)\n",
        "        self.masks.append(mask)\n",
        "\n",
        "    def clear(self):\n",
        "        self.states.clear()\n",
        "        self.actions.clear()\n",
        "        self.values.clear()\n",
        "        self.log_probs.clear()\n",
        "        self.rewards.clear()\n",
        "        self.masks.clear()\n"
      ],
      "metadata": {
        "id": "smIBgLMS5bMp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example uses the CartPole-v1 environment from OpenAI Gym. In this code:\n",
        "\n",
        "We first define a function collect_experience_and_train that will be responsible for collecting experiences and training the agent.\n",
        "Within the function, we loop for a given number of episodes.\n",
        "For each episode, we iterate for a set number of timesteps or until the episode ends.\n",
        "At each timestep, we use the policy to sample an action, take the action in the environment, and then store the experience in memory.\n",
        "After the episode is over, we use the collected experiences in memory to train the PPO agent.\n",
        "After training, we clear the memory to prepare for the next episode.\n",
        "Finally, we demonstrate how to use the function with the CartPole-v1 environment.\n",
        "\n",
        "** In Detail **\n",
        "\n",
        "Imports:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import gym\n",
        "We're using the gym library, which provides a suite of environments for reinforcement learning. In this example, we're using the CartPole-v1 environment.\n",
        "\n",
        "Function Definition - collect_experience_and_train:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "def collect_experience_and_train(agent, memory, env, num_episodes, max_timesteps=1000):\n",
        "This function collects experiences and trains the agent.\n",
        "\n",
        "Parameters:\n",
        "agent: The PPO agent we're training.\n",
        "memory: The memory object to store experiences.\n",
        "env: The environment we're interacting with.\n",
        "num_episodes: Number of episodes for training.\n",
        "max_timesteps: Maximum number of timesteps in each episode.\n",
        "Episode Loop:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "We loop through the specified number of episodes. For each episode, we reset the environment to get the initial state and set the episode reward to zero.\n",
        "\n",
        "Timestep Loop:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "for t in range(max_timesteps):\n",
        "For each episode, we have another loop to iterate through timesteps. This can either run for max_timesteps or end earlier if the episode is terminated (e.g., the cartpole falls).\n",
        "\n",
        "Policy Evaluation & Action Selection:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "    action_dist, value = agent.policy(state_tensor)\n",
        "    action = action_dist.sample().item()\n",
        "    log_prob = action_dist.log_prob(torch.tensor([action]))\n",
        "We convert the current state to a PyTorch tensor.\n",
        "We pass the state through the policy network to get the action distribution and value.\n",
        "We sample an action from the action distribution and compute its log probability.\n",
        "Environment Step:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "next_state, reward, done, _ = env.step(action)\n",
        "We take the selected action in the environment. This returns the next state, the reward for the action, and a flag done indicating if the episode ended (e.g., the pole fell).\n",
        "\n",
        "Store Experience:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "memory.append(state, action, value, log_prob, reward, not done)\n",
        "We store the experience (state, action, value, log probability, reward, and continuation flag) in memory.\n",
        "\n",
        "Update State and Reward:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "episode_reward += reward\n",
        "state = next_state\n",
        "We update the cumulative episode reward with the new reward and set the current state to the next\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tt0u8LFb7N4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "def collect_experience_and_train(agent, memory, env, num_episodes, max_timesteps=1000):\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        for t in range(max_timesteps):\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                action_dist, value = agent.policy(state_tensor)\n",
        "                action = action_dist.sample().item()\n",
        "                log_prob = action_dist.log_prob(torch.tensor([action]))\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            memory.append(state, action, value, log_prob, reward, not done)\n",
        "\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        print(f\"Episode {episode + 1} Reward: {episode_reward}\")\n",
        "\n",
        "        # After collecting experiences for an episode, train the agent\n",
        "        agent.train(memory)\n",
        "\n",
        "        # Clear the memory after training\n",
        "        memory.clear()\n",
        "\n",
        "env_name = \"CartPole-v1\"\n",
        "env = gym.make(env_name)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "agent = PPO(state_dim, action_dim, continuous=False)\n",
        "memory = Memory()\n",
        "\n",
        "collect_experience_and_train(agent, memory, env, num_episodes=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unXfIzxT5or9",
        "outputId": "452fcc8c-d34a-4f91-cc3a-a75a110c0e99"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1 Reward: 20.0\n",
            "Episode 2 Reward: 20.0\n",
            "Episode 3 Reward: 15.0\n",
            "Episode 4 Reward: 48.0\n",
            "Episode 5 Reward: 17.0\n",
            "Episode 6 Reward: 37.0\n",
            "Episode 7 Reward: 26.0\n",
            "Episode 8 Reward: 20.0\n",
            "Episode 9 Reward: 16.0\n",
            "Episode 10 Reward: 43.0\n",
            "Episode 11 Reward: 13.0\n",
            "Episode 12 Reward: 13.0\n",
            "Episode 13 Reward: 16.0\n",
            "Episode 14 Reward: 29.0\n",
            "Episode 15 Reward: 30.0\n",
            "Episode 16 Reward: 26.0\n",
            "Episode 17 Reward: 10.0\n",
            "Episode 18 Reward: 32.0\n",
            "Episode 19 Reward: 36.0\n",
            "Episode 20 Reward: 24.0\n",
            "Episode 21 Reward: 42.0\n",
            "Episode 22 Reward: 17.0\n",
            "Episode 23 Reward: 15.0\n",
            "Episode 24 Reward: 16.0\n",
            "Episode 25 Reward: 40.0\n",
            "Episode 26 Reward: 18.0\n",
            "Episode 27 Reward: 26.0\n",
            "Episode 28 Reward: 18.0\n",
            "Episode 29 Reward: 14.0\n",
            "Episode 30 Reward: 13.0\n",
            "Episode 31 Reward: 17.0\n",
            "Episode 32 Reward: 10.0\n",
            "Episode 33 Reward: 17.0\n",
            "Episode 34 Reward: 20.0\n",
            "Episode 35 Reward: 13.0\n",
            "Episode 36 Reward: 17.0\n",
            "Episode 37 Reward: 44.0\n",
            "Episode 38 Reward: 40.0\n",
            "Episode 39 Reward: 59.0\n",
            "Episode 40 Reward: 25.0\n",
            "Episode 41 Reward: 14.0\n",
            "Episode 42 Reward: 13.0\n",
            "Episode 43 Reward: 70.0\n",
            "Episode 44 Reward: 14.0\n",
            "Episode 45 Reward: 22.0\n",
            "Episode 46 Reward: 21.0\n",
            "Episode 47 Reward: 17.0\n",
            "Episode 48 Reward: 23.0\n",
            "Episode 49 Reward: 15.0\n",
            "Episode 50 Reward: 11.0\n",
            "Episode 51 Reward: 42.0\n",
            "Episode 52 Reward: 22.0\n",
            "Episode 53 Reward: 21.0\n",
            "Episode 54 Reward: 18.0\n",
            "Episode 55 Reward: 22.0\n",
            "Episode 56 Reward: 12.0\n",
            "Episode 57 Reward: 21.0\n",
            "Episode 58 Reward: 75.0\n",
            "Episode 59 Reward: 19.0\n",
            "Episode 60 Reward: 28.0\n",
            "Episode 61 Reward: 32.0\n",
            "Episode 62 Reward: 22.0\n",
            "Episode 63 Reward: 39.0\n",
            "Episode 64 Reward: 11.0\n",
            "Episode 65 Reward: 14.0\n",
            "Episode 66 Reward: 14.0\n",
            "Episode 67 Reward: 40.0\n",
            "Episode 68 Reward: 71.0\n",
            "Episode 69 Reward: 56.0\n",
            "Episode 70 Reward: 29.0\n",
            "Episode 71 Reward: 19.0\n",
            "Episode 72 Reward: 42.0\n",
            "Episode 73 Reward: 36.0\n",
            "Episode 74 Reward: 9.0\n",
            "Episode 75 Reward: 25.0\n",
            "Episode 76 Reward: 34.0\n",
            "Episode 77 Reward: 22.0\n",
            "Episode 78 Reward: 39.0\n",
            "Episode 79 Reward: 17.0\n",
            "Episode 80 Reward: 30.0\n",
            "Episode 81 Reward: 45.0\n",
            "Episode 82 Reward: 25.0\n",
            "Episode 83 Reward: 17.0\n",
            "Episode 84 Reward: 42.0\n",
            "Episode 85 Reward: 12.0\n",
            "Episode 86 Reward: 54.0\n",
            "Episode 87 Reward: 37.0\n",
            "Episode 88 Reward: 25.0\n",
            "Episode 89 Reward: 31.0\n",
            "Episode 90 Reward: 19.0\n",
            "Episode 91 Reward: 21.0\n",
            "Episode 92 Reward: 36.0\n",
            "Episode 93 Reward: 23.0\n",
            "Episode 94 Reward: 22.0\n",
            "Episode 95 Reward: 15.0\n",
            "Episode 96 Reward: 57.0\n",
            "Episode 97 Reward: 17.0\n",
            "Episode 98 Reward: 15.0\n",
            "Episode 99 Reward: 17.0\n",
            "Episode 100 Reward: 55.0\n"
          ]
        }
      ]
    }
  ]
}