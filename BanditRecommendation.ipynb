{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP+1w1yWaJA+1f41fRnVCeU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijaygwu/classideas/blob/main/BanditRecommendation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkZIKdAbIfJK",
        "outputId": "f81f70ad-e379-499f-e51a-52947af77616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated values (click-through rates) for the articles: [0.02380952 0.03910615 0.02020202 0.03759398 0.03149606 0.04130163\n",
            " 0.02824859 0.02459016 0.03703704 0.02755906]\n",
            "Number of times each article was recommended: [ 294.  358.   99.  133.  127. 7990.  177.  244.  324.  254.]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class EpsilonGreedy:\n",
        "    def __init__(self, n_arms, epsilon=0.1):\n",
        "        self.n_arms = n_arms\n",
        "        self.epsilon = epsilon\n",
        "        self.counts = np.zeros(n_arms)\n",
        "        self.values = np.zeros(n_arms)\n",
        "\n",
        "    def select_arm(self):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.randint(self.n_arms)\n",
        "        else:\n",
        "            return np.argmax(self.values)\n",
        "\n",
        "    def update(self, chosen_arm, reward):\n",
        "        self.counts[chosen_arm] += 1\n",
        "        n = self.counts[chosen_arm]\n",
        "        value = self.values[chosen_arm]\n",
        "        self.values[chosen_arm] = ((n - 1) / n) * value + (1 / n) * reward\n",
        "\n",
        "# Simulated recommendation using Epsilon-Greedy\n",
        "n_articles = 10  # Let's assume we have 10 news articles\n",
        "bandit = EpsilonGreedy(n_articles, epsilon=0.1)\n",
        "\n",
        "n_users = 10000\n",
        "\n",
        "# Simulated click-through rates for the 10 articles\n",
        "# These are hidden in real life, but we simulate them for this example\n",
        "click_through_rates = [0.02, 0.03, 0.015, 0.04, 0.025, 0.035, 0.02, 0.03, 0.05, 0.04]\n",
        "\n",
        "for _ in range(n_users):\n",
        "    recommended_article = bandit.select_arm()\n",
        "    reward = np.random.choice([0, 1], p=[1-click_through_rates[recommended_article], click_through_rates[recommended_article]])\n",
        "    bandit.update(recommended_article, reward)\n",
        "\n",
        "print(f\"Estimated values (click-through rates) for the articles: {bandit.values}\")\n",
        "print(f\"Number of times each article was recommended: {bandit.counts}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using bandit algorithms for recommendation systems can optimize the recommendations based on user engagement. In this example, I'll again use the Epsilon-Greedy algorithm for simplicity, but in practice, you might consider more sophisticated bandit approaches or even context-aware bandits.\n",
        "\n",
        "Let's assume we're building a content recommendation system for a news website:\n",
        "\n",
        "Setup the experiment:\n",
        "\n",
        "Each article or news piece is considered as an \"arm\" of the bandit.\n",
        "Our reward will be 1 if a user clicks on the recommended article and 0 otherwise.\n",
        "Simulate user interactions:\n",
        "\n",
        "For the sake of this example, we'll simulate user interactions. In a real-world scenario, you'd integrate the bandit algorithm with your recommendation engine to serve articles and observe user clicks."
      ],
      "metadata": {
        "id": "_edVazaKItWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, 10,000 users are interacting with our recommendation system. The bandit algorithm will try to learn which articles are more engaging (higher click-through rate) based on user interactions and will then recommend those more frequently.\n",
        "\n",
        "In a real-world scenario, you would replace the simulation logic with actual user interactions. Whenever a user visits the news homepage, you'd use select_arm to decide which article to prominently recommend, and user click interactions would provide the \"reward\" to the update method."
      ],
      "metadata": {
        "id": "NgJNVPWMIyw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a real-world scenario, the click-through rates (CTRs) are not known a priori. Instead, they are inferred from user interactions with the recommendations over time. The bandit algorithm helps estimate these CTRs through iterative updates.\n",
        "\n",
        "In the code I provided above, the values array in the EpsilonGreedy class is essentially the algorithm's current estimate of the CTR for each article (or arm). Initially, this estimate is set to zero for all articles. As users are shown articles and either click (or don't click) on them, these estimates get updated.\n",
        "\n",
        "Here's how the CTR estimation works in the real world using the bandit algorithm:\n",
        "\n",
        "Initialization: Initially, each article has no data, so the estimated CTR is 0.\n",
        "\n",
        "Recommendation: When a user visits the platform, the recommendation system uses the select_arm function to choose an article to recommend. This might be the article with the highest estimated CTR or a randomly chosen one, depending on the epsilon-greedy logic.\n",
        "\n",
        "User Interaction: The user either clicks on the recommended article (reward = 1) or doesn't (reward = 0).\n",
        "\n",
        "Update Estimates: The update function is called with the article index and the reward. This function updates the estimated CTR for that article based on the new data. Over time and with more user interactions, these estimates will converge closer to the true CTRs of the articles.\n",
        "\n",
        "Iterate: Steps 2-4 are repeated for each user interaction, allowing the algorithm to continuously refine its CTR estimates and its recommendations.\n",
        "\n",
        "In a real-world application, here's how you could capture user interactions and update the bandit:"
      ],
      "metadata": {
        "id": "accvTGJbI3lW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the bandit is already defined and initialized\n",
        "bandit = EpsilonGreedy(n_articles, epsilon=0.1)\n",
        "\n",
        "# A user visits the platform\n",
        "recommended_article = bandit.select_arm()\n",
        "\n",
        "# Show the recommended_article to the user...\n",
        "\n",
        "# If the user clicks on the article:\n",
        "bandit.update(recommended_article, 1)\n",
        "\n",
        "# If the user doesn't click on the article:\n",
        "# bandit.update(recommended_article, 0)\n"
      ],
      "metadata": {
        "id": "LAhXVmTvJT-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The true CTR for each article is essentially the total number of clicks divided by the total number of times the article was shown. The bandit algorithm provides its own estimate of this rate, which is stored in the values attribute. The advantage of using the bandit approach over traditional methods is that it can simultaneously learn about the CTRs of the articles while also optimizing to show users the best articles."
      ],
      "metadata": {
        "id": "3GsXEoefJWKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gmBFPWiLJafT"
      }
    }
  ]
}